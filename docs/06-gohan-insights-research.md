# gohan (@grandchildrice) の実践知見に基づく調査レポート

## 概要

本ドキュメントは、7ヶ月で500万円以上を稼いだスマートコントラクト監査者 gohan (@grandchildrice) のツイートから得られた実践知見を核に、AIエージェントによるバグバウンティ/監査の方向性を検討するための調査レポートである。

**🏆 重要な成果 (2025年12月):** gohan率いるNyx FoundationのAIエージェントが、イーサリアム「Fusaka」監査コンテストで**17件の脆弱性を発見し、報告件数で世界1位を獲得**した。

---

## 1. gohan の実績と背景

### プロフィール
- **X(Twitter)**: @grandchildrice
- **GitHub**: github.com/grandchildrice
  - 110+ リポジトリ
  - 組織: NyxFoundation, zk-tokyo
  - 専門: Ethereum, Cryptography, zkvm-benchmarks
- **Sherlock ランキング**: #151
  - 獲得額: $21,352 USDC
  - ポイント: 34 pts
  - アクティブ日数: 28日

### 所属組織: Nyx Foundation
- **URL**: https://nyx.foundation
- **スポンサー**: イーサリアム財団、Geode Labs、GuildQB 他
- **研究領域**:
  - 分散システム・暗号技術の研究開発
  - ホワイトハッキング（バグバウンティ・脆弱性研究）
  - 形式検証（Lean4による暗号プロトコル検証）
  - AIエージェント開発

### 時系列
- **2024年5月**: スマートコントラクト監査開始
- **2024年12月**: 累計500万円以上達成（約7ヶ月）
- **2025年9月**: Nyx Foundation 法人設立
- **2025年11月**: AIエージェントでLean形式検証を自動化
- **2025年12月24日**: Fusaka監査コンテストで世界1位獲得
- **最初の2ヶ月**: 31個のソースコードを監査

---

## 2. gohan の失敗経験（重要）

### ❌ 失敗1: 最初からバグバウンティに挑戦

> 「最初Bug Bountyから攻めた → 失敗」

**理由の分析:**
- バグバウンティの対象は**本番稼働中のプロダクト**
- すでに複数の監査を通過済み → 「バグが出し尽くされた後」
- 難易度が最大化された状態でのバグ探し
- 初心者には成功体験を得にくい

### ❌ 失敗2: LangChain でゴリゴリ自動化

> 「最初からLangChainでゴリゴリ自動化しようとしたり、失敗も当然ありました」

**推測される失敗要因:**

1. **ドメイン知識の不足**
   - 監査の本質を理解しないまま自動化を試みた
   - 何を自動化すべきかの判断ができなかった

2. **LLMの限界**
   - スマートコントラクトのコンテキスト理解の限界
   - ハルシネーション（誤検出/見逃し）
   - 論理的なバグの検出は静的解析の方が信頼性が高い

3. **End-to-End自動化の難しさ**
   - 監査は「コードを読む → 理解する → 攻撃ベクトルを考える → テストで検証」の複合作業
   - 単純なパイプラインでは対応できない

4. **フィードバックループの欠如**
   - 手動監査の経験なしに自動化システムの評価ができない
   - 何が「良い検出」かの基準がない

---

## 3. gohan の成功戦略

### ✅ 成功の転換点: 監査コンテストから入る

> 「監査コンテストから入ることをおすすめします」

**監査コンテストの利点:**
1. **時間制限内での集中的な学習**
2. **他の参加者のレポートから学べる**
3. **成功体験を得やすい（相対的に発見しやすいバグが残っている）**
4. **報酬が保証されている（プール制）**

### ✅ 具体的な学習アプローチ

gohanが挙げた4つの方法:

1. **他人のレポートを読みまくる**
   - Solodit, Code4rena reports, Sherlock findings
   - 高品質なレポートの書き方を学ぶ
   - 攻撃パターンの知識を蓄積

2. **コンペに参加しまくる**
   - 最初の2ヶ月で31個のソースコードを監査
   - 量をこなすことで「目」を養う

3. **コードを読みまくる**
   - プロトコルの設計思想を理解
   - パターン認識能力の向上

4. **テストを書きまくる**
   - PoC（Proof of Concept）作成能力
   - 仮説検証のサイクル

---

## 4. AIエージェントの4つのアプローチ（gohan による分類）

gohanは以下の4つのアプローチを試したと述べている:

### 1. 攻撃ベクタを推論する方法
- LLMに「このコードの攻撃ベクトルは？」と聞く
- **課題**: ハルシネーション、コンテキスト制限

### 2. テスト生成しまくる方法
- LLMにファジングテストや単体テストを生成させる
- **有望**: 比較的成功しやすいアプローチ

### 3. 静的解析ツール使う方法
- Slither, Aderyn などの出力を活用
- **推奨**: ツール vs 人間の研究で機械が得意な領域が判明

### 4. コードを自然言語に起こして意味論的にバグ探索
- コードをNLに変換 → セマンティック検索
- **gohanの結論**: これが最もうまくいった

> 「4番目の方法が今のところ一番うまくいっています」

---

## 5. 監査コンテスト vs バグバウンティ 比較分析

| 項目 | 監査コンテスト | バグバウンティ |
|------|--------------|---------------|
| **対象** | 監査前のコード | 本番稼働中のコード |
| **競争** | 時間制限内の参加者同士 | 世界中の常時参加者 |
| **難易度** | 中〜高 | 最高 |
| **バグの残存率** | 比較的高い | 低い（出し尽くされている） |
| **報酬形態** | プール分配 | 発見時のみ |
| **学習価値** | 高（レポート公開） | 低（非公開が多い） |
| **初心者向け** | ◯ | ✕ |

### 主要プラットフォーム

**監査コンテスト:**
- [Code4rena](https://code4rena.com/reports) - 450+件のレポート公開
- [Sherlock](https://audits.sherlock.xyz/contests) - Watson ranking システム
- [CodeHawks](https://codehawks.com/) - Cyfrin 運営

**バグバウンティ:**
- [Immunefi](https://immunefi.com/) - 最大規模
- [Hats Finance](https://hats.finance/) - 分散型

---

## 6. 高品質レポートの情報源

### Solodit (solodit.cyfrin.io)
- **50,000件以上**の脆弱性データベース
- Code4rena, Sherlock, Cyfrin 等のレポートを集約
- 脆弱性タイプ別検索可能
- **AI エージェント開発に最適なデータソース**

### Code4rena Reports
- 2021年以降の全レポート公開
- 最新: Megapot, Sequence, Flare, LayerZero 等
- プロトコル別、日付別で検索可能

### Sherlock Findings
- 各コンテストの発見事項を公開
- 重大度別の分類
- ジャッジのコメント付き

---

## 7. arXiv の最新研究（LLM × スマートコントラクトセキュリティ）

69件の関連論文が存在（2024-2025年）:

### 注目論文

1. **Smart-LLaMA-DPO** (ISSTA 2025)
   - DPO強化学習を用いた説明可能な脆弱性検出

2. **SAEL: Mixture-of-Experts** (ICSME 2025)
   - 適応的MoEによる脆弱性検出

3. **SymGPT**
   - 記号実行 + LLM のハイブリッドアプローチ

4. **SmartAuditFlow** (Plan-Execute Framework)
   - Plan-Execute パターンでのハルシネーション対策

5. **LLAMA: Multi-Feedback Fuzzing**
   - LLM誘導シード生成によるファジング

6. **ParaVul: RAG Framework**
   - 並列LLM + RAGによる検出

### 研究から見える傾向

1. **ハイブリッドアプローチが主流**
   - LLM単独ではなく、静的解析/形式検証との組み合わせ

2. **専門化されたファインチューニング**
   - 汎用LLMではなく、Solidity特化モデル

3. **説明可能性の重視**
   - なぜ脆弱性かを説明できる能力

---

## 8. Cyfrin セキュリティコース構成（学習ロードマップ参考）

Cyfrin の50時間以上のセキュリティコースカリキュラム:

### Phase 1: 基礎
1. PasswordStore Audit - 最初の監査
2. Puppy Raffle Audit - 静的解析、Reentrancy、弱いRNG

### Phase 2: DeFi
3. TSwap Audit - インバリアント、ファジング、AMM
4. Thunder Loan Audit - プロキシ、オラクル、Flash Loan

### Phase 3: 高度
5. Boss Bridge Audit - ブリッジ、署名、Assembly
6. Vault Guardians Audit - MEV、ガバナンス、ERC-4626

### 主要な脆弱性パターン
- DoS (Denial of Service)
- Reentrancy
- Weak RNG
- Arithmetic Issues
- Poor ETH Handling
- Access Control
- Oracle Manipulation
- Signature Replay
- Storage Collision

---

## 9. AIエージェント開発への示唆

### gohan の経験からの教訓

1. **まず手動監査を学ぶ**
   - 自動化の前にドメイン知識が必須
   - 何を自動化すべきかの判断力を養う

2. **段階的なアプローチ**
   - 全自動 → ✕
   - 人間を補助するツール → ◯

3. **「セマンティック検索」アプローチが有望**
   - コードをNL化 → ベクトル検索 → 類似バグパターン発見

### 推奨される開発順序

```
Phase 1: 手動監査の習熟（2-3ヶ月）
├── 監査コンテスト参加（Code4rena, Sherlock）
├── 他者のレポートを500件以上読む
└── 30+ コードベースを監査

Phase 2: ツール補助システム開発
├── 静的解析結果の整理・優先順位付け
├── コードのNL化・サマリー生成
└── 類似脆弱性の検索システム

Phase 3: 半自動化
├── テスト/PoC 自動生成
├── 攻撃ベクトル候補の提案
└── レポートドラフト生成

Phase 4: より高度な自動化（将来）
├── インバリアント自動推論
├── ファジング戦略の自動設計
└── マルチエージェント監査
```

---

## 10. Nyx Foundation AIエージェントの成果（2025年12月）

### 🏆 世界1位達成の衝撃

**2025年12月24日、Nyx FoundationのAIエージェントがイーサリアム「Fusaka」監査コンテストにおいて、17件の脆弱性を発見し、報告件数で世界1位を獲得した。**

これは「LangChain自動化が失敗した」という初期の経験を乗り越え、正しいアプローチでAIエージェントが実世界で成果を出せることを実証した歴史的な成果である。

### AIエージェントアーキテクチャ

gohanが構築したAIエージェントシステムの特徴（SCIS2026発表資料より）:

#### 全体アーキテクチャ：熟練監査員の思考を自動化

```
┌───────────────────────────────────────────────────────────────────┐
│              AI Security Agent Architecture                       │
├───────────────────────────────────────────────────────────────────┤
│                                                                    │
│  ┌──────────────────────────────────────────────────────────────┐ │
│  │           Phase 1: 準備（知識の構造化）                       │ │
│  │  ┌──────────────┐  ┌───────────────┐  ┌──────────────────┐   │ │
│  │  │ 仕様書(EIP)  │→│  実装コード   │→│チェックリスト    │   │ │
│  │  │ 要件抽出     │  │  マッピング   │  │類似バグリスト   │   │ │
│  │  └──────────────┘  └───────────────┘  └──────────────────┘   │ │
│  │                                                               │ │
│  │  出力: 01_SPEC.json                                           │ │
│  │  ├─ architecture.components: 10件                             │ │
│  │  ├─ security_requirements: 8件                                │ │
│  │  └─ threats.attack_paths: 12件                                │ │
│  └──────────────────────────────────────────────────────────────┘ │
│                               ↓                                    │
│  ┌──────────────────────────────────────────────────────────────┐ │
│  │           Phase 2: 監査（3戦略の並行実行）                    │ │
│  │                                                               │ │
│  │   ┌────────────────┐ ┌────────────────┐ ┌────────────────┐   │ │
│  │   │(i) 仕様ベース  │ │(ii) 類似バグ   │ │(iii) 動的      │   │ │
│  │   │    静的検査    │ │    探索 ⭐     │ │   テスト生成   │   │ │
│  │   │   3件(17.6%)   │ │  13件(76.5%)   │ │   1件(5.9%)    │   │ │
│  │   └────────────────┘ └────────────────┘ └────────────────┘   │ │
│  │                                                               │ │
│  │  出力: 03_AUDITMAP.json                                       │ │
│  │  ├─ id: "P2P-001", risk: "DoS-Memory"                         │ │
│  │  └─ status: "ok" | "needs-review"                             │ │
│  └──────────────────────────────────────────────────────────────┘ │
│                               ↓                                    │
│  ┌──────────────────────────────────────────────────────────────┐ │
│  │                      レポート出力                             │ │
│  └──────────────────────────────────────────────────────────────┘ │
│                                                                    │
└───────────────────────────────────────────────────────────────────┘
```

#### 3つの戦略の詳細

| 戦略 | 説明 | 発見数 | 比率 |
|------|------|--------|------|
| **(i) 仕様ベース静的検査** | 「設計図と実物を見比べる」EIP仕様と実装の整合性検証 | 3件 | 17.6% |
| **(ii) 類似バグ探索 ⭐** | 「過去の失敗から学ぶ」既知バグパターンの横展開 | 13件 | **76.5%** |
| **(iii) 動的テスト生成** | 「実際に動かして試す」テスト/Fuzzer自動生成 | 1件 | 5.9% |

> **💡 最も効果を発揮したのは「類似バグ探索」**
> 発見された脆弱性の**76.5% (13/17件)**がこの戦略によるもの

#### 類似バグ探索の仕組み

```json
// bugs_ethereum.json - 既知バグパターンDB
{
  "CVE-2023-42319": "GraphQL DoS",
  "CL-2023-01": "BlocksByRange溢れ"
}
→ 類似パターンを全クライアントで探索
```

#### Fusakaコンテスト結果詳細

| 指標 | 値 |
|------|-----|
| 総報告数 | 54件 |
| 有効判定 | 17件 |
| 有効率 | 31.5% |
| High | 0件 |
| Medium | 0件 |
| Low | 1件 |
| Info | 16件 |

#### 監査の民主化：非専門家が最多発見

| 監査者 | 経験 | 発見数 |
|--------|------|--------|
| D氏 | **エンジニア未経験** | **8件（最多）** |
| B氏 | ホワイトハッカー | 7件 |
| A氏 | シニアエンジニア | 1件 |
| C氏 | ジュニアエンジニア | 1件 |

> **特筆点**: エンジニアやハッカーの経験がないD氏が最多のバグを発見
> → AIエージェントによる「監査の民主化」を実証

#### 形式検証の自動化（Lean4）
- 2025年11月: 暗号プロトコルのLean形式検証をAIエージェントで自動化
- GitHub: NyxFoundation/tsl-formal-verification

### 成功の鍵: 段階的アプローチ

gohanの失敗→成功の軌跡:

| フェーズ | 時期 | アプローチ | 結果 |
|---------|------|-----------|------|
| 1 | 初期 | Bug Bountyから攻める | ❌ 失敗 |
| 2 | 初期 | LangChainゴリゴリ自動化 | ❌ 失敗 |
| 3 | 中期 | 監査コンテスト + 手動 | ✅ 成功 (500万円+) |
| 4 | 後期 | AIエージェント + ドメイン知識 | ✅ **世界1位** |

### 重要な教訓

> **「LangChain自動化は失敗した」≠「AI自動化は不可能」**

正しい教訓:
- ドメイン知識なしの「ゴリゴリ自動化」は失敗する
- 手動経験を積み、ドメイン知識を獲得してから自動化すると成功する
- マルチエージェント + セマンティック分析 + 静的解析の組み合わせが有効

---

## 11. 結論と次のアクション

### 重要な発見

1. **バグバウンティよりも監査コンテストから始めるべき**
   - 学習効率が圧倒的に高い
   - 成功体験を得やすい

2. **LangChain「ゴリゴリ自動化」は失敗した（初期段階で）**
   - ドメイン知識なしの自動化は機能しない
   - 段階的なアプローチが必要
   - **しかし、ドメイン知識を獲得した後のAI自動化は成功する（世界1位を達成）**

3. **セマンティック検索アプローチが有望**
   - gohanの4つのアプローチのうち最も成功
   - Nyx Foundation AIエージェントの核心技術

4. **ハイブリッドシステムが主流かつ実証済み**
   - LLM + 静的解析 + ファジングの組み合わせ
   - Nyx Foundationが17件の脆弱性発見で実証

5. **🆕 AIエージェントは実世界で成果を出せる**
   - 2025年12月: Fusaka監査コンテストで世界1位
   - 形式検証（Lean4）との統合も進行中

### 次のステップ

1. [ ] 監査コンテストに参加して手動監査を体験（ドメイン知識獲得）
2. [ ] Solodit のデータを活用したセマンティック検索システムのプロトタイプ
3. [ ] 静的解析ツール（Slither, Aderyn）の出力を整理するシステム
4. [ ] PoC/テスト自動生成の実験
5. [ ] 🆕 Nyx Foundation の GitHub リポジトリ（scis2026-security-agent-presentation）を研究
6. [ ] 🆕 マルチエージェントアーキテクチャの設計

---

## 参考リンク

- [Solodit - Vulnerability Database](https://solodit.cyfrin.io/)
- [Code4rena Reports](https://code4rena.com/reports)
- [Sherlock Contests](https://audits.sherlock.xyz/contests)
- [Cyfrin Security Course](https://updraft.cyfrin.io/courses/security)
- [SC Exploits Minimized](https://github.com/Cyfrin/sc-exploits-minimized)
- [devdacian AI Auditor Primers](https://github.com/devdacian/ai-auditor-primers)
- **[Nyx Foundation](https://nyx.foundation/)** - gohanが主宰する研究組織
- **[NyxFoundation GitHub](https://github.com/NyxFoundation)** - AIエージェント関連リポジトリ
- **[tsl-formal-verification](https://github.com/NyxFoundation/tsl-formal-verification)** - Lean4形式検証
